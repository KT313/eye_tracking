{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "113b0d21-db4a-4bf6-8fbd-0796ece6bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3942ec7f-a239-4668-ba54-7a470677a570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobi/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from lib.imports import *\n",
    "from lib.video_processor import VideoProcessor\n",
    "from lib.misc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e42670c-0193-4a77-82f2-2c6b7dadf5fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.9 üöÄ Python-3.10.12 torch-2.2.1+cu121 CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "WARNING ‚ö†Ô∏è half=True only compatible with GPU export, i.e. use device=0\n",
      "YOLOv8n-pose summary (fused): 187 layers, 3154073 parameters, 0 gradients, 8.6 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models/eyes_256_auto.pt' with input shape (3, 3, 256, 256) BCHW and output shape(s) (3, 35, 1344) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.36...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 0.5s, saved as 'models/eyes_256_auto.onnx' (12.1 MB)\n",
      "\n",
      "Export complete (1.8s)\n",
      "Results saved to \u001b[1m/home/tobi/Desktop/eye_tracking/models\u001b[0m\n",
      "Predict:         yolo predict task=pose model=models/eyes_256_auto.onnx imgsz=256  \n",
      "Validate:        yolo val task=pose model=models/eyes_256_auto.onnx imgsz=256 data=data/yolo_pose_dataset.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Loading models/yolov8n-face-lindevs.onnx for ONNX Runtime inference...\n",
      "Loading models/eyes_256_auto.onnx for ONNX Runtime inference...\n"
     ]
    }
   ],
   "source": [
    "interpolate_rate=5\n",
    "eyes_int_rate=1\n",
    "eye_model_size=256\n",
    "input_source = 0 # \"example_video/example_vid_original.mp4\"\n",
    "form=\"onnx\"\n",
    "\n",
    "\n",
    "eye_batch_rate=(interpolate_rate//(eyes_int_rate+1))+1\n",
    "convert_pt_to_onnx(model_path=f'models/eyes_{eye_model_size}_auto.pt', imgsz=eye_model_size, batch=eye_batch_rate, form=form)\n",
    "\n",
    "processor = VideoProcessor(eye_model_size=eye_model_size, form=form)\n",
    "out = processor.start(video_path=input_source, interpolate_rate=interpolate_rate, eyes_int_rate=eyes_int_rate, eye_batch_rate=eye_batch_rate, return_instead=False, disable_stats=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23da502-c30f-43bc-8d2e-1d1682709209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_video(out, \"example_video/flexpressions_cropped.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403489e0-3925-4614-a38f-cadbbf3d9a38",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875eb63-d894-47ce-809b-a3a466606694",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_video_annotations_for_training(val_ratio=0.1, multiply_train_data=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc9d85-ed80-4d29-970c-98df08e77342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't want to track training with comet don't run this cell and set usecomet=False in the next cell\n",
    "init_comet(api_key=\"your_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd4300-4a05-4608-914e-ceb6382ec721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for image_size in [64, 128, 256, 512]: # might get stuck before starting 2nd batch, if this happens just restart this cell\n",
    "    train_model(pretrained_model='models/yolov8n-pose.pt', save_name=f\"training_{image_size}_autotrain\", imgsz=image_size, epochs=50, batch=64, workspace=\"comet_workspacename\", usecomet=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
